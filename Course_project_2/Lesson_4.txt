__________________________________________________________________________________________________________________________________
1. Расскажите, как работает регуляризация в решающих деревьях, какие параметры мы штрафуем в данных алгоритмах?
__________________________________________________________________________________________________________________________________

Регуляризация (англ. regularization) в статистике, машинном обучении, теории обратных задач — метод добавления некоторых 
дополнительных ограничений к условию с целью решить некорректно поставленную задачу или предотвратить переобучение.
Чаще всего эта информация имеет вид штрафа за сложность модели.
Однин из способов бороться с негативным эффектом излишнего подстраивания под данные — использование регуляризации, 
т. е. добавление некоторого штрафа за большие значения коэффициентов у линейной модели. 
Тем самым запрещаются слишком "резкие" изгибы, и предотвращается переобучение.
Переобучение в большинстве случаев проявляется в том, что итоговые модели имеют слишком большие значения параметров. 
Соответственно, необходимо добавить в целевую функцию штраф за это. 
Наиболее часто используемые виды регуляризации — L1 и L2, а также их линейная комбинация — эластичная сеть.
Регуляризация — одна из эвристик улучшения градиентных методов обучения. Основным способом уменьшить переобучение является 
квадратичная регуляризация, называемая также сокращением весов. Чтобы ограничить рост абсолютных значений весов, 
к минимизируемому функционалу Q(w) добавляется штрафное слагаемое:

Qτ(w)=Q(w)+τ2∥w∥2
Это приводит к появлению аддитивной поправки в градиенте:

Q′τ(w)=Q'(w)+τw
В результате правило обновления весов принимает вид:

w:=w(1−ητ)−ηQ′(w)
Таким образом, вся модификация сводится к появлению неотрицательного множителя (1−ητ), приводящего к постоянному уменьшению весов.

Регуляризация предовтращает паралич, повышает устойчивость весов в случае мультиколлинеарности, повышает обобщающую способность алгоритма 
и снижает риск переобучения. Однако есть и недостатки — параметр τ необходимо выбирать с помощью кросс-валидации, что связано с большими 
вычислительными затратами.
Регуляризация также используется и в нейронных сетях для борьбы со слишком большими весами сети и переобучением. 
Однако, в этом случае зануление коэффициентов при использовании L1-регуляризатора не несет в себе смысл "отбора признаков", 
как в случае с линейными моделями. К сожалению, регуляризация не снижает число параметров и не упрощает структуру сети.

Для нейронной сети помимо добавления штрафного слагаемого к эмпирическому риску активно используют и другой метод борьбы с переобучением — 
прореживание сети (англ. dropout), в ходе которого упрощают сеть, руководствуясь правилом — если функция ошибки не изменяется, 
то сеть можно упрощать и дальше.

__________________________________________________________________________________________________________________________________
2. По какому принципу рассчитывается "важность признака (feature_importance)" в ансамблях деревьев?
__________________________________________________________________________________________________________________________________

Отдельные деревья решений можно легко интерпретировать, просто визуализировав древовидную структуру. 
Однако модели повышения градиента состоят из сотен деревьев регрессии, поэтому их нельзя легко интерпретировать путем 
визуального осмотра отдельных деревьев. К счастью, был предложен ряд методов для обобщения и интерпретации моделей повышения градиента.

Часто функции не в равной степени способствуют предсказанию целевой реакции; во многих ситуациях большинство функций фактически 
не имеют значения. При интерпретации модели первым вопросом обычно является: каковы эти важные особенности и как они влияют на 
прогнозирование целевого ответа?

Отдельные деревья решений по сути выполняют выбор функций, выбирая соответствующие точки разделения. 
Эта информация может использоваться для измерения важности каждой функции; основная идея такова: чем чаще функция используется 
в точках разделения дерева, тем важнее эта функция. Это понятие важности может быть распространено на ансамбли деревьев решений 
путем простого усреднения важности характеристик каждого дерева на основе примесей.
