________________________________________________________________________________________________________________________________________
1. Для чего и в каких случаях полезны различные варианты усреднения для метрик качества классификации: micro, macro, weighted?
________________________________________________________________________________________________________________________________________

F1-оценка: это индекс, используемый для измерения точности модели двоичной классификации в статистике, и он используется для измерения
точности несбалансированных данных. Он также принимает во внимание точность и отзывчивость модели классификации. 
F1-оценку можно рассматривать как средневзвешенное значение точности и запоминаемости модели. Максимальное значение - 1, минимальное - 0.

В задаче множественной классификации, если вы хотите рассчитать F1-оценку модели, есть два метода расчета, а именно микро-F1 и макро-F1.
Эти два метода расчета аналогичны методам расчета F1-оценки в двух классификациях.Так, в задаче с двумя категориями вычислить 
микро-F1 = макро-F1 = F1-оценка, микро-F1 и макро-F1 - это два метода расчета мультикатегорийной F1-оценки.

Два метода расчета F1-балла по нескольким категориям:

micro-F1：
Диапазоны：(0, 1)；
Применимая среда: Мульти-категория несбалансирована, если данные крайне несбалансированы, это повлияет на результаты;

TPi означает, что истинно положительный класс i-го класса оценивается как положительный класс;
FPi означает, что ложноположительный отрицательный класс i-го класса оценивается как положительный класс;
TNi относится к истинно отрицательному i-й категории, а положительная категория оценивается как отрицательная категория;
FNi означает, что ложноотрицательный результат i-й категории считается отрицательной категорией.
за micro-Recall: Скорость отзыва составляет (TP1 + TP2 + TP3) / (TP1 + TP2 + TP3 + FN1 + FN2 + FN3), то есть три категории TP и FN 
складываются вместе в знаменателе, а TP - в числителе. Из анализа приведенной выше формулы мы видим, 
что TP1 + TP2 + TP3 = a + e + i; FN1 + FN2 + FN3 = b + c + d + f + g + h (то есть притяжательные, кроме TP).
для micro-Precision： Точность составляет (TP1 + TP2 + TP3) / (TP1 + TP2 + TP3 + FP1 + FP2 + FP3), можно получить то же самое, 
TP1 + TP2 + TP3 = a + e + i; FP1 + FP2 + FP3 = d + g + b + h + c + f (т.е. притяжательные, кроме TP).
И для точности модели Accuracy, определяется как доля правильно классифицированных образцов во всех выборках. Итак, формула точности
Acc=(a+e+i)/(a+b+c+d+e+f+g+h+i)
Исходя из этого, мы можем сделать вывод: если micro-F1 = 0,5, точность модели Acc = 0,5, Precision и Recall равны 0,5,
но мы не можем сделать вывод, что модель в основном зависит от предположения Вывод.
(a+e+i)/(a+b+c+d+e+f+g+h+i)=micro-F1=Precision_mi=Recall_mi=0.5

macro-F1：
Диапазоны：(0, 1)；
Применимая среда: Проблема с множественной классификацией, не подверженная дисбалансу данных, легко затрагивается категориями с
высокой узнаваемостью (высокая отзывчивость, высокая точность);
макро-F1 имеет два метода расчета:
1. Сначала найдите макрос-Recall и macro-Pecision, а затем найдите макро-F1 из суммы этих двух;
2. Непосредственно усредните F1-балл по трем категориям.
В пакете sklearn используется второй метод.Использование этих двух методов всегда было спорным, но в «Алгоритмах обучения для
линейных классификаторов текста (Льюис, Дэвид Д. и др.« Алгоритмы обучения для классификаторов линейного текста »»). 
В "SIGIR. Vol. 96. 1996.)" автор указал, что макро-F1 - это среднее значение F1-балла во всех классах, то есть второй способ так
рассчитывается макро-F1.
Мы также можем проанализировать два метода усреднения. Первый метод не очень чувствителен к распределению ошибок ».
См. Статью (Opitz, Juri, and Sebastian Burst.« Macro F1 and Macro F1. »ArXiv preprint arXiv: 1911.03347 (2019 )) »,
Это немного похоже на микро-F1,
второй способ рекомендован авторами указанных статей.
Аналогичным образом мы анализируем каждую категорию показателей:
Для первой категории: FP1 = d + g; TP1 = a; FN1 = b + c; TN1 = e + f + h + i;
Для категории 2: FP2 = b + h; TP2 = e; FN2 = d + f; TN2 = a + c + g + i;
Для категории 3: FP3 = c + f; TP3 = i; FN3 = g + h; TN3 = a + b + d + e;
Если значение = 0,5, то полезного заключения сделать нельзя.

weighted-F1
Помимо микро-F1 и макро-F1, существует также взвешенный-F1, который является результатом умножения F1-балла на долю этого типа и
последующего сложения.
Его также можно рассматривать как вариант макро-F1.
Фактически, из вышесказанного также видно,weighted-F1Он не равен вычисленному с помощью взвешенной точности и взвешенного отзыва.
Weighted-F1(Здесь для того, чтобы выразить соответственно заглавными буквами) по той же причине относятся к двум методам вычисления
макроса. В общем первый как результат.

________________________________________________________________________________________________________________________________________
2. В чём разница между моделями xgboost, lightgbm и catboost или какие их основные особенности?
________________________________________________________________________________________________________________________________________

Light GBM — это быстрая, распределенная, высокопроизводительная структура повышения градиента, основанная на алгоритме дерева решений,
используемая для ранжирования, классификации и многих других задач машинного обучения.
Поскольку он основан на алгоритмах дерева решений, он разделяет лист дерева с наилучшим соответствием, тогда как другие алгоритмы повышения
делят дерево по глубине или уровню, а не по листу. Таким образом, при выращивании на одном и том же листе в Light GBM, листовой алгоритм
может уменьшить больше потерь, чем поуровневый алгоритм, и, следовательно, приводит к гораздо лучшей точности, что редко может быть
достигнуто любым из существующих алгоритмов повышения.

Преимущества Light GBM:
 Быстрая скорость обучения и высокая эффективность: Легкий GBM использует алгоритм на основе гистограммы, то есть он объединяет непрерывные
значения признаков в дискретные ячейки, которые ускоряют процедуру обучения.
 Более низкое использование памяти: Заменяет непрерывные значения на дискретные ячейки, что приводит к снижению использования памяти.
 Лучшая точность, чем у любого другого алгоритма усиления: Он создает гораздо более сложные деревья, следуя подходу с разбивкой по листам,
а не по уровням, что является основным фактором в достижении более высокой точности. Однако иногда это может привести к переоснащению,
которого можно избежать, установив параметр max_depth.
 Совместимость с большими наборами данных: Он способен одинаково хорошо работать с большими наборами данных при значительном сокращении
времени обучения по сравнению с XGBOOST.

XGBoost — алгоритм машинного обучения, основанный на дереве поиска решений и использующий фреймворк градиентного бустинга.
В задачах предсказания, которые используют неструктурированные данные (например, изображения или текст), искусственная нейронная сеть превосходит
все остальные алгоритмы или фреймворки. Но когда дело доходит до структурированных или табличных данных небольших размеров,
в первенстве оказываются алгоритмы, основанные на дереве поиска решений.

Особенности фреймворка:
 Широкая область применения: может быть использован для решения задач регрессии, классификации, упорядочения и пользовательских задач на предсказание.
 Совместимость: Windows, Linux и OS X.
 Языки: поддерживает большинство ведущих языков программирования, например, C++, Python, R, Java, Scala и Julia.
 Облачная интеграция: поддерживает кластеры AWS, Azure и Yarn, хорошо работает с Flink, Spark.

CatBoost — это библиотека градиентного бустинга, созданная Яндексом. Она использует небрежные (oblivious) деревья решений, чтобы вырастить
сбалансированное дерево. Одни и те же функции используются для создания левых и правых разделений (split) на каждом уровне дерева.

Есть несколько причин подумать об использовании CatBoost:
 CatBoost позволяет проводить обучение на нескольких GPU.
 Библиотека позволяет получить отличные результаты с параметрами по умолчанию, что сокращает время, необходимое для настройки гиперпараметров.
 Обеспечивает повышенную точность за счет уменьшения переобучения.
 Возможность быстрого предсказания с применением модели CatBoost;
 Обученные модели CatBoost можно экспортировать в Core ML для вывода на устройстве (iOS).
 Умеет под капотом обрабатывать пропущенные значения.
 Может использоваться для регрессионных и классификационных задач.